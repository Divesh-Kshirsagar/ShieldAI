"""
SHIELD AI — Phase 1: Data Ingestion & Real-Time Cleaning
=========================================================

Provides two Pathway tables consumed by downstream modules:

    cetp_stream        : Clean CETP inlet readings (floats, NA rows dropped).
    factory_raw_stream : All factory rows with BLACKOUT-tagged NA rows retained (v2).

Column naming approach
----------------------
Pathway 0.29.x requires Schema field names to exactly match CSV column headers.
Since the raw MPCB file (priya_cetp_i.csv) uses long headers with spaces and
hyphens, simulate_factories.py first creates:
    - data/cetp/cetp_clean.csv     (short Pythonic headers)
    - data/factories/factory_*.csv (short Pythonic headers)

Pathway reads from these preprocessed files. The raw file is never modified.

Cleaning rules
--------------
1. CETP stream  : Drop any row where cetp_inlet_cod is empty/null.
2. Factory stream: Retain null-cod rows but tag them status='BLACKOUT' (v2 anti-cheat).

Pathway streaming mode
----------------------
mode='streaming' tails the CSV directory and picks up new rows as they arrive,
replicating a live MPCB feed when historical CSVs are played back sequentially.
"""

import json
import logging
import time
from pathlib import Path

import pathway as pw

from src.config import CONFIG as _cfg
from src.validation import validate_record

log = logging.getLogger(__name__)

_CETP_DATA_DIR:    str = _cfg.cetp_data_directory
_FACTORY_DATA_DIR: str = _cfg.factory_data_directory


# ---------------------------------------------------------------------------
# Schema definitions
# ---------------------------------------------------------------------------

# NOTE: Field names must match cetp_clean.csv headers EXACTLY.
# cetp_clean.csv is generated by simulate_factories.preprocess_cetp().
class CETPSchema(pw.Schema):
    """Schema for data/cetp/cetp_clean.csv."""

    # NOTE: s_no is not primary_key — Pathway auto-generates unique IDs from
    # (file_path, row_position). Using s_no as PK would be fine for a single
    # CSV but we keep it consistent with FactorySchema (see below).
    s_no:             int
    time:             str
    cetp_inlet_cod:   float | None
    cetp_inlet_bod:   float | None
    cetp_inlet_ph:    float | None
    cetp_inlet_tss:   float | None
    cetp_outlet_cod:  float | None
    cetp_outlet_bod:  float | None
    cetp_outlet_ph:   float | None
    cetp_outlet_tss:  float | None


# NOTE: Field names must match factory_*.csv headers EXACTLY.
# Factory CSVs use clean names: s_no, time, factory_id, cod, bod, ph, tss.
class FactorySchema(pw.Schema):
    """Schema for data/factories/factory_*.csv."""

    # NOTE: s_no MUST NOT be primary_key here. All 4 factory CSVs share the
    # same s_no range (1-18781). If marked as PK, Pathway would see 4 × 18781
    # duplicate keys when reading the directory, causing 'duplicated entries'
    # errors in the asof/interval join engine. Pathway auto-generates unique
    # row IDs from (filename, row_position) instead, which are globally unique.
    s_no:        int
    time:        str
    factory_id:  str
    cod:         float | None
    bod:         float | None
    ph:          float | None
    tss:         float | None


class QuarantineSchema(pw.Schema):
    """Schema for invalid records routed to the quarantine stream."""

    record:           str    # Original record as JSON string
    rejection_reason: str    # Why it was rejected
    received_at:      float  # Wall-clock arrival time (Unix epoch)



# ---------------------------------------------------------------------------
# CETP stream
# ---------------------------------------------------------------------------

def load_cetp_stream(
    cetp_dir: str = _CETP_DATA_DIR,
    return_quarantine: bool = False
) -> pw.Table | tuple[pw.Table, pw.Table]:

    """Read the preprocessed CETP CSV in streaming mode and return a clean inlet stream.

    Reads data/cetp/cetp_clean.csv (generated by simulate_factories.preprocess_cetp).
    Drops rows where cetp_inlet_cod is null (sensor gap / raw NA row).

    Args:
        cetp_dir: Directory containing cetp_clean.csv.

    Returns:
        Pathway Table matching CETPSchema with null-COD rows removed.
    """
    clean_path = Path(cetp_dir) / "cetp_clean.csv"
    if not clean_path.exists():
        raise FileNotFoundError(
            f"Preprocessed CETP file not found: '{clean_path}'.\n"
            "Run 'uv run python src/simulate_factories.py' first to generate it."
        )

    # NOTE: No value_columns / column_names kwargs — Pathway 0.29.x uses Schema
    # field names to match CSV headers directly. Headers in cetp_clean.csv
    # already match CETPSchema field names after preprocessing.
    raw: pw.Table = pw.io.csv.read(
        str(clean_path),        # read a single file, not a directory
        schema=CETPSchema,
        mode="streaming",
        autocommit_duration_ms=1_000,
    )

    # Step 2: Apply Input Validation
    # CETPSchema doesn't have sensor_id, so we inject 'CETP_INLET'
    # wrap_with_validation expects 'time' and 'cetp_inlet_cod' columns.
    with_sensor = raw.with_columns(sensor_id="CETP_INLET")

    valid_stream, quarantine_stream = wrap_with_validation(
        with_sensor,
        sensor_col="sensor_id",
        value_col="cetp_inlet_cod"
    )

    # Filter: keep only rows where the CETP inlet COD sensor fired (non-null)
    # NOTE: wrap_with_validation already rejects None values as invalid per rules.
    # We re-filter just to be safe and match original logic.
    cetp_clean: pw.Table = valid_stream.filter(pw.this.cetp_inlet_cod.is_not_none())

    if return_quarantine:
        return cetp_clean, quarantine_stream
    return cetp_clean



# ---------------------------------------------------------------------------
# Validation Wrapper
# ---------------------------------------------------------------------------

def wrap_with_validation(
    stream: pw.Table,
    sensor_col: str,
    value_col: str,
) -> tuple[pw.Table, pw.Table]:
    """Validate a stream and split it into valid and quarantine tables.

    Standardises the stream to (sensor_id, timestamp, value) before calling
    validate_record.

    Args:
        stream:     Input Pathway Table (must have sensor_col, 'time', value_col).
        sensor_col: Name of the column identifying the sensor.
        value_col:  Name of the column carrying the measurement.

    Returns:
        (valid_stream, quarantine_stream)
    """

    @pw.udf
    def udf_validate(sensor_id: str, timestamp: str, value: float | None) -> tuple[bool, str]:
        record = {
            "sensor_id": sensor_id,
            "timestamp": timestamp,
            "value":     value,
        }
        return validate_record(record)

    @pw.udf
    def udf_log_rejection(reason: str, sensor_id: str, value: float | None) -> None:
        log.warning(
            "record quarantined",
            extra={
                "rejection_reason": reason,
                "sensor_id":        sensor_id,
                "value":            value,
            }
        )

    # 1. Project to standard validation format
    # We keep the original row but add validation results
    v_res = stream.select(
        original_row=pw.this,
        v_tuple=udf_validate(
            getattr(pw.this, sensor_col),
            pw.this.time,
            getattr(pw.this, value_col)
        )
    ).with_columns(
        is_valid=pw.this.v_tuple[0],
        reason=pw.this.v_tuple[1],
    )

    # 2. Split into valid and quarantine
    valid_raw = v_res.filter(pw.this.is_valid).select(pw.this.original_row)
    # Flatten the row back to original schema
    valid_stream = valid_raw.select(*valid_raw.original_row)

    quarantine_raw = v_res.filter(~pw.this.is_valid)

    # Side-effect: log the rejection
    # We use a dummy column to trigger the UDF
    quarantine_with_log = quarantine_raw.with_columns(
        _log=udf_log_rejection(
            pw.this.reason,
            getattr(pw.this.original_row, sensor_col),
            getattr(pw.this.original_row, value_col)
        )
    )

    @pw.udf
    def udf_to_json(row: dict) -> str:
        return json.dumps(row, default=str)

    quarantine_stream = quarantine_with_log.select(
        record=udf_to_json(pw.this.original_row),
        rejection_reason=pw.this.reason,
        received_at=time.time(),
    )

    return valid_stream, quarantine_stream


# ---------------------------------------------------------------------------
# Factory streams
# ---------------------------------------------------------------------------

def load_factory_streams(
    factory_dir: str = _FACTORY_DATA_DIR,
    return_quarantine: bool = False
) -> pw.Table | tuple[pw.Table, pw.Table]:

    """Read all factory CSVs in the directory in streaming mode.

    Reads all factory_*.csv files in factory_dir. Each factory is already tagged
    with factory_id inside the CSV (written by simulate_factories.py).

    Retains null-COD rows tagged as status='BLACKOUT' for v2 anti-cheat logic.

    Args:
        factory_dir: Directory containing factory_A/B/C/D.csv.

    Returns:
        Pathway Table with FactorySchema columns plus a `status` string column.
    """
    if not Path(factory_dir).exists():
        raise FileNotFoundError(
            f"Factory data directory not found: '{factory_dir}'.\n"
            "Run 'uv run python src/simulate_factories.py' first."
        )

    # Step 1: Read raw CSVs
    raw: pw.Table = pw.io.csv.read(
        str(factory_dir),
        schema=FactorySchema,
        mode="streaming",
        autocommit_duration_ms=1_000,
    )

    # Step 2: Apply Input Validation & Schema Enforcement
    valid_stream, quarantine_stream = wrap_with_validation(
        raw,
        sensor_col=_cfg.input_schema_sensor_column,
        value_col=_cfg.input_schema_value_column
    )

    # Tag each row: NORMAL vs BLACKOUT (using the validated stream)
    factory_with_status: pw.Table = valid_stream.with_columns(
        status=pw.if_else(
            pw.this.cod.is_not_none(),
            "NORMAL",
            "BLACKOUT",
        )
    )

    if return_quarantine:
        return factory_with_status, quarantine_stream
    return factory_with_status


def load_clean_factory_stream(factory_dir: str = FACTORY_DATA_DIR) -> pw.Table:
    """Return the factory stream with BLACKOUT (null-COD) rows removed.

    Used by Phase 1 aggregate → backtrack pipeline where only valid
    float COD readings participate in the asof_join.

    Args:
        factory_dir: Directory containing factory CSV files.

    Returns:
        Pathway Table with only NORMAL factory rows.
    """
    full = load_factory_streams(factory_dir)
    return full.filter(pw.this.cod.is_not_none())
